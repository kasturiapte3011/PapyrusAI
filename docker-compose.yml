version: "3.9"

services:
  llm:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llm
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:ro
    command: >
      --model /models/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf
      --ctx-size 4096
      --n-gpu-layers 35
      --threads 8
      --host 0.0.0.0
      --port 8080
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 2s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  backend:
    build: ./backend
    container_name: rag-backend
    ports:
      - "8000:8000"
    environment:
      - USE_LOCAL_LLM=true
      - LLM_URL=http://llm:8080
    volumes:
      - ./data:/app/data
    depends_on:
      llm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/ready')"]
      interval: 5s
      timeout: 2s
      retries: 5
  
  frontend:
